%
% Sample SBC book chapter
%
% This is a public-domain file.
%
% Charset: ISO8859-1 (latin-1) áéíóúç
%
\documentclass{SBCbookchapter}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazil,english]{babel}
\usepackage{graphicx}
\usepackage{array}

\setcounter{chapter}{1}

\author{Sam Haynes}
\title{Introduction}

\begin{document}
\maketitle

\begin{abstract}
This chapter introduces open-source software practices, community development and the pedagogy of software documentation to promote the development of long-term, high impact research software. Focusing on the issue of the standardisation of qPCR analysis, the chapter contrasts the R package tidyqpcr, developed by the author, to other current software available. This use case highlights how quality software supported by comprehensive documentation can improve the quality of the entire experimental assay.


\end{abstract}

The acquisition of quality, high-throughput data sets is outpacing the creation and implementation of reproducible and statistically rigorous analysis methods. 

Although more statistical methods are being developed now more than ever, the advance skills in programming, statistics and domain specific biological knowledge is creating a divide larger than ever.

Emphasis needs to shift not only in creating new methods but also providing the documentation and training to correctly implement these methods.

Advanced analysis algorithms (deep learning or hierarchical inference) does not equate to better scientific research or knowledge.

Only though correct implementation of statistical methods can we actually make progress.

Moving from low-hanging fruit to more complex regulatory mechanisms requires more complex but rigorous analysis methods.

Avoiding the reproducibility crisis as more data is available then ever, specifically in qPCR but spreading to other assay? Was it there for micro arrays? Is it appearing for RNA-Seq?

Normalising is tough as we explore more complex problems.

Proper statistical methods implemented correctly is the way forward.

tidyqpcr: Takling the reproducibility crisis in qPCR results by building an intuitive R package based on the MIQE-guideline and best practices in software development.

3'UTR project: More complex but interpretable models of gene expression are required to achieve the next degree of precision in controling gene expression

Diff Frac: Alternative methods for normalisation for different biological assays overcoming spike-ins and DESeq-2

1) Technology is out-pacing theory
2) Fancy statistical methods is not enough
3) Implementing quality software development and documentation can solve problems.

tidyqpcr: Poor experimental design, quality control and transparency is leading to a reproducibility crisis in biology.

Despite detailed papers outlining best practices and numerous analysis packages there remains a significant lack of rigour in qPCR results.

Incorporating open software development practices together with experimental design and QC can help solve this.

3’UTR paper: Cis-regulatory elements display complex multi-dimension roles depending on sequence context, localisation and cell environment.

Synthetic biology requires more precise models of gene expression to accelerate development of novel synthetic pathways.

Fundamental cell biology needs new statistical tools to explore complex regulatory behaviour.

Diff Frac: Normalisation of RNA-Seq data to enable comparison across experiments remains a non-trivial task. 

DESeq2, TPM, spike-ins, and normalising genes all have assumptions that can often be broken.

Modelling errors using Bayesian statistical models and implementing quality experimental design can overcome many issues.

\section{Quantification of mRNA datasets}

\subsection{RNA-seq}
RNA-Seq analysis consists of three core steps: Quality Control, alignment and counting. The exact quality control steps can change significantly between types of assay. For example, enrichment of mRNA transcripts using degradation or poly(A) anchors need to be checked for effectiveness by inspecting ribosomal RNA content or tRNA levels. Meanwhile in the case of single-cell RNA-Seq checking for cases where two or more cells may have accidentally been combined (as doublets are common in occurrence in many technique) is a vital step that is not required for bulk RNA-seq methods. However, across all methods it will be required to check for sequence amplification biases, calling quality and whether each lane has successfully detect reads. Once QC has been completed it is then important to remove any adapters and UMI that may have been introduced during the library preparation as these may complicate alignment to the organism genome as they will not be included. Also it is common to trim the 3' end of reads as more error in nucleotide callings tend to occur at then end. If the assay also includes multiplexed samples these need to detect and separated. Once the reads have been trimmed and multiplexed, then they need to be aligned to the organism genome in order to be able to determine which gene they correlate to. A variety of genome aligners are available depending on organism and computing infrastructure limitations.  BowTie2 is regularly held as the most accurate aligner but it is not splice aware. Other aligner like star or hi-sat2 are much better at aligning reads across splicing junctions. There is a significant difference in speed and memory requirements between the two and users need to decide what is the priority for them. It is vital that quality control steps are conducted after the alignment step. Are the vast majority of reads aligned to your organism genome or could a contaimenant be present. Visualing reads on a genome browers is important at this point too to check you have correctly defined strandedness. Once the sequences have been aligned the next step is to remove PCR duplicates if UMIs are present. If the UMIs with random sequences are present in reads aligned to the exactly the same gene then they are considered duplicates and can be flatten to just one read. Finally, with the deduplicated aligned reads fully processed the user can then count the number of reads to each gene or conduct other downstream analyses such as detecting isoforms.

Reproducible analysis of RNA-seq data is crucial as software versions, computer systems and servers can all contribute to different outcomes. Using workflow management programs, such as NextFlow, also enable scaleability (analysis of results on a computer to using a university server or cloud service). The analysis steps will be the same but the number of core/input files/ repeats of steps will change mostly without the need of user input. Combining workflow languages with containers: singularity, docker, ect, or environments: conda, renv, ect, and enabling your analysis scripts to be publicly available simultaneously allows you to conduct the same (or similar) analysis in the future and encourages reviewers to actualy confirm you analysis is correct and that there are no issue with QC.

\subsection{qPCR}

Quantitative PCR is among the most common techniques in biological and biomedical research used for the quantification of DNA and RNA. It is considered the gold standard for nucleotide detection and quantification in medicine, legislation and academia. Its prominence has never more apparent than through its use in curtailing the spread of SARS-CoV-2 with a reliable qPCR assay published within 2 weeks of the first release of the SARS-CoV-2 genome \cite{Li2020}. Nevertheless, the widespread usage of qPCR across distinct disciplines has inhibited the education in and implementation of best-practices and quality control. \cite{Bustin2021} From sample preparation to statistical analysis, each stage of a qPCR experiment can be customised to extract information about numerous biological processes. Developing equally versatile and reliable software built with best-practices and using education by documentation it could enable path to overcome the lack of consistency across the disjoint disciplines. 

The basic principle of a polymerase chain reaction consists of the duplication of a region on DNA or RNA that is specified by two short nucleotide sequences, called primers, that are designed to be complementary to the start and the end of the region of interest. A highly thermal tolerant polymerase, adapted from the bacteria \textit{Thermus aquaticus}, is then able to complete the duplication of the region by elongation of the sequence between the two primers \cite{Holland1991}. The PCR polymerase must be thermal tolerant as the duplication step is rapidly repeated by raising the temperature to melt the newly created complementary stand away from the original strand before dropping the temperature back down to enable the next round of elongation. If this cycle is repeated in a solution with surplus concentrations of primers, enzymes, and nucleotides the increase in copies per cycle is bound by an exponential defined only by the number of transcripts of the target sequence in the original sample. Therefore, if the exponential growth of PCR duplicates can be measured and accurately modelled then the original number of transcripts can be inferred. To measure the exponential duplication, Quantitative PCR introduces dyes that only fluoresce when a region has been successfully duplicated. The fluorescence of the sample is measured as the PCR cycle is repeated to determine the exponential growth in duplicates and quantify the number of transcripts of the target sequence in the original sample.

\subsubsection{qPCR Methods: RNA vs DNA}

\begin{itemize}
    \item qPCR assays are highly optimised for \textit{Thermus aquaticus} polymerase
    \item RNA samples can be analysed with qPCR machines but must be reverse transcribed to cDNA
    \item The addition reverse transcription step is regularly determined to be the source of most variation between RNA samples \cite{Stahlberg2004}
    \item RT priming method, original RNA target concentration and total RNA concentration in the sample can all vary the cDNA yield between replicates.
\end{itemize}

\subsubsection{qPCR Methods: Taqman vs SYBR Green}

# figure outlining different methods


The fluorescent dye used to detect duplicates differs significantly between the two most common qPCR methods: Taqman and SYBR Green. SYBR Green methods contain dye that fluoresce when they bind to any double stranded DNA species in the sample. This leads it to being a cheap and relatively easy to use system, but it is highly susceptible to contamination and it is unable to distinguish between duplicates from different species. TaqMan methods bind the fluorescent dye to a oligonucleotide probe that is designed to attach to the region of interest somewhere between the two primers. In addition to the fluorescent dye on one end the oligonucleotide probe has a quencher on the other. The quencher stops any emissions from the dye from being detected. However, during elongation, when the polymerase reaches the oligonucleotide probe, the probe is hydrolysed separating the dye from the quencher. Emissions from the fluorescent dye can then be measured and the creation of a new duplicate is detected. The introduction of a custom oligonucleotide probe increases the specificity of TaqMan methods and reduces the effects of contamination. Also, the abundance of multiple species can be measured in the same sample by carefully designing different oligonucleotide probes with different fluorescent dyes. Unfortunately, the design and creation of custom probes causes TaqMan methods to be more expensive and technical. Meanwhile, the accuracy of TaqMan methods is comparable to the cheaper SYBR Green methods, if correctly conducted. \cite{Tajadini2014}

\subsubsection{Quantifying Abundance: Curve Fitting vs Cycle Threshold}

The exponential limit of the duplicate number enables two methods that summarise amplification curves to compare abundance across samples. Assuming all samples reach the exponential growth stage in copy number per cycle at the same time then the difference it fluorescence at any cycle of the PCR assay dependent only on the original copy number. Therefore, if you set a particular fluorescence threshold during the exponential phase to compare all samples at, the number of cycles needed to reach that fluorescence level is an accurate proxy for original copy number. A common method to determine target abundance is the number of cycles required to reach a set fluorscence. Unfortunately this method assumes both that each sample reaches the exponential phase at the same time and that each cycle doubles the number of duplicates perfectly for each sample. An alternative method fits an sigmodal curve to the amplification curve and uses this model to deduce the cycles required to reach a the threshold. The additional fitting can account for difference in the times to reach the exponential growth phase between samples and can directly account for deviations in perfect duplication. 

\subsubsection{Quantifying Abundance: Relative vs Absolute}

Multiple methods exist for converting fluorescence measurements into quantitative values for sample abundance whilst accounting for experimental and technical noise. First, qPCR experiments can be designed either to measure the relative change in abundance across samples or estimate the absolute number of copies of a target in a sample. Relative abundance measurements depend on the determination of genes that have constant expression across all samples/conditions. Any change in the gene(s) of interest across samples can then be detected by comparing expressions relative to the set of constantly expressed genes. Normalising the fluorescence to genes with constant expression minimises batch effects introduced by sample preparation, reverse transcription and reactants. Absolute quantification of a target requires a preliminary experiment where known initial quantities of the gene of interest are measured with qPCR. The amplification curve for each sample, with gradually increasing copies of the gene of interest, is measured to create a collection of standard curves. Next, the sample from the primary experiment is measured with qPCR and its amplification curve is compared to the standard curves. The absolute copy number of the gene of interest in the experimental sample can be interpolated.

# table of assumptions for each method

There is a critical need for rigorous analysis and reporting of qPCR experiments, codified in the minimum information for publication of quantitative real-time PCR experiments (MIQE) guidelines \cite{Bustin2009, Bustin2017, Courts2019}. Yet it is common for qPCR to be analysed either by closed-source software supplied by the manufacturers of PCR machines, or by highly variable, in-house analysis scripts that have not been peer-reviewed.


\section{Research Software Engineering}

The development of high-throughput, multi-omic experiments across the biological sciences has led to an unprecedented demand on software for research. In the late 90's less than 20\% of research papers mentioned the use of software in their research. In 2021, over 70\% of publications stored on pubmed cited the use of software. Furthermore, articles accepted in the highest impact journals cited more pieces of software on average. (sentence emphasising use of statistical software). Software developed specifically to answer research questions has rapidly become a cornerstone of the modern empirical method. \cite{Schindler2022}

Despite its importance, research software development remains a nascent subdomain of software development. The academic position of research software engineer was only coined in the late 2000's \cite{Prause2010} with the creation of the society of software engineers being founded in 2010. Exemplary research groups and institutions exist with the primary aim of developing software to answer, or facilitate others in answering, research questions (SSI etc). However, it still remains a domain primarily occupied by self-taught programmers as an aside from their primary domain of expertise.  The software itself is regularly developed with specific short term goals during the latter stages of scientific inquiry to provide statistical summaries and deduce significant discoveries. Therefore, practitioners are rarely trained in the standard development practices that are used across all other professional software development contexts and they often consider the analysis steps a final hurdle before completing a project/publication.

This leads to common patterns that define research software. Research software is typically written in high level scripting languages (R/Python), often to produce statistical summaries of complex datasets as a means to gather evidence to support a conclusion in the latter stages of scientific inquiry (typically. during the steps of writing a publication for submission).

\subsection{Software development practices}

As the practice of software development became a profession, practitioners have created frameworks to facilitate efficient and rigorous software development.
The first software development framework, and the one still most used in the sciences, is called Waterfall.
Waterfall introduces a structure to the coding practice by outlining a series of stages that are completed linearly.
It starts with a detailed specification before moving to development and finally implementation.
Alternative practices generally deviate from the waterfall ethos of leaving implementation to the very last stage.
Instead they prioritise creating a minimal viable product as soon as possible and testing its functionality.
Others even enable the specification to be altered and redefined as the project develops.
The flexibility to redefine the project as it is being developed is why the alternative practices are called agile. 

Two common agile practices in industry are scrum and extreme programming.
Scrum is the modern archetypal agile method.
Instead of fixing the development schedule as each stage much be completed sequentially the project is broken into sprints each iteratively adding some functionality which is reviewed tested and implemented before moving onto the next.
Constantly reviewing and testing the code enables programmers to catch bugs early (ensuring complex dependency issues don't arise) receive feedback on whether initial functionality is actually useful and achievable.
As its name suggests extreme programming pushes the principles of agile programming to the extreme
Updates to the software are expected on a weekly basis with additions tested and implemented. 
In addition programmers are expected to conduct paired programming. 
Two programmes work together at all time with only one coding while the other verbally dictates what should be added. 
Although this reduces the number of lines of code written per developer, with two coders constantly reviewing each other code the number of programming bugs introduced is reduced which is assumed to negate any reduction in productivity.

Does research software differ from general software? (in terms of aims, requirements and expectations)

Software development methodologies from Waterfall to Extreme focus entirely on efficient and accurate frameworks for writing code. 
Although these methodologies differ in the timing and extent of feedback from primary stakeholders none explicitly state time for writing documentation. 
Software is only as effective as its user.
Coding practices are highly developed. Efficient coding practices, interactions between teams and automatic integration and testing of work. In practice, software is limited by the users understanding of its implementation.
Documentation practices are less developed.

% https://scrumguides.org/docs/scrumguide/v2020/2020-Scrum-Guide-US.pdf#zoom=100

% https://hygger.io/blog/extreme-programming-waterfall-agile-kanban-scrum-lean/#extreme-programming-vs-waterfall

% https://ieeexplore.ieee.org/abstract/document/1231158?casa_token=6sbQr1xrIy4AAAAA:0GmEQG8RPMLhd5563BpXhyfQqSPd7Tw88hrhpX2uo4bu3LgewtZNrNKTYggK0JMFJpBvsFMUS2c

% https://peerj.com/articles/cs-835/#results--analysis-of-software-mentions

\subsubsection{Computation and the environment}

A fast growing aspect of software development practices is the contribution of compute to the emission of CO2.
It is estimated that the IT sector will be responsible for 20\% of the world's CO2 production by 2030. 
Incorporating carbon footprint estimates into coding specifications and benchmarking published code may be the norm in the future
Balancing efficiencies and speed for new software updates may need to be encouraged.
Depending on highly inefficient older code may be detrimental to the carbon footprint but the latest version my prioritise speed over memory/CPU usage and actually increase emissions.
In addition, programming philosophies may need to be developed. 
Time tends to be the limiting factor in many analyses so swapping to faster code is nearly always the priority. 
However, is there a limit to how much we prioritise speed? 
If each run emits more CO2 and we conduct far more runs than was initially thought necessary will that always be seen as the ideal.

Two common sequence aligners hi-sat 2 and star are noted for their significant difference in memory consumption.
Previously research has noted that memory usage is overtaking CPU as the leading energy consumer in severs.
Over allocating memory to a task can significantly increase energy usage with 90\% of energy spent on maintaining memory in the background regardless of load.
Interestingly, the consequences of agile development practises with weekly or even daily testing could contribute to significant increases CO2 emissions.
Tools such of GitHub actions do enable quick detection of bugs but does the significant increase in compute worth the cost?

% https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009324

% https://dl.acm.org/doi/pdf/10.1145/3076113.3076117?download=true

\subsection{Open source software development}

The growing expectation for a science that has its results open to all to read, data freely available to use and decisions that are completely transparent leads to an natural proclivity towards the already developed world of open software. The software development practices described above can be followed with a closed development process or an open one. It is fairly prolific across research to leave any code used in a research paper to be 'available upon request'. However, the encouragement of open research software practices offers many of the advantages currently seen in industry. The success of the Google supported Machine Learning framework TensorFlow showcases the enhanced re-usability, collaboration and accessibility enabled by adompting an Open Source framework. Specific to the academic setting,  it have been shown suggest that publications that follow open science practices tend to have increased citations. A modern parable for open source software development comes for the epidemiological modelling of the spread of COVID-19 by Prof Neil Ferguson at Imperial Collage London. Crucial to the justification of national lockdowns to kerb the spread of COVID-19, the model was actual developed 13 years prior using undocumented, closed source C++ code. After six weeks of intense revisions and refactoring, with direct help from Microsoft and GitHub software architects,  is now, arguably, the poster child for open source research software.

% https://github.com/mrc-ide/covid-sim

% https://www.nature.com/articles/nature10836?em_x=22

% https://elifesciences.org/articles/16800

\subsection{GUIs or CLIs}

\begin{itemize}
    \item Since the macintosh there has been a continued shift between GUI and CLI statistical software taking the lead \cite{ValeroMora2012}. 
    \item GUIs tend to be associated a dramatically lower learning curve but larger development and maintenance costs.
    \item Better user experience in a quality GUI than CLI \cite{Staggers2000}, however do GUI developers prioritise ease of use over quality of analysis by ignoring key steps (such as quality control).
    \item GUIs are also limited by the number of ways a user can interact with the screen (interactive operations), drop down boxes quickly become unwieldy if they list every possible action to take on an object. 
    \item CLIs are often quicker to develop (so can include the latest statistical methods), easier to scale to larger work loads, better reproducibility and cheaper to maintain.
    \item Without a unified framework to conduct statistical analysis (how you define models and manipulate data) there cannot be an intuitive structure to create and expand a GUI. \cite{Unwin2012}

\end{itemize}

\section{Software documentation}

\subsection{Why is research software documentation important}

Software is intended to be a tool used to complete tasks efficiently.
It alleviates some of the mundane and repetitive tasks to enable users to focus on the complex and abstract. 
However, the intent is not always matched by the execution as users may become frustrated with design idiosyncrasies or suboptimal implementations.
In fact, researchers often dread the steps in their experimental assays that require the use of software or the analysis stages of their studies.
An opinion quickly becoming untenable as the trend trend towards data rich, high throughput studies has led even the most rudimentary analyses to depend on software.
Indeed, as statistical methodologies are still being developed to explore the vast data landscape the practice of reanalysing old experiments is only going to be more common. 
Although the demands for better computational literacy in undergrad and postgrad education is partially addressing these issues only by demanding software meet users halfway by emphasising transparency and accessibility will the problem full resolved.

In order to support reproducibility and the user experience, research software needs to be rewarded on its usability and documentation as well as its functionality.
In order assess the quality of software documentation it is useful to consider the types of documentation broadly separated by its intended audience and content. 
Previous studies have recognised three categories of documentation: documentation of decisions, why did you chose this problem to solve and why did you chose this particular method to solve it; documentation of product, what is contained within this software implementation and how do users interact with it; and documentation of technicalities, how developers created the software and how maintainers can contribute to it. 
Any software intended to be shared contains some product documentation, but few research software projects outline technical details and fewer still mention any decisions made in development \cite{Geiger2018}.

Funding bodies and journals have acknowledged across the board that research data needs to be FAIR (Findable, Accessible, Interoperable and Reusable). \textbf{evidence on uptake of fair practices} However, few guidelines have been implemented on the software packages and analysis pipelines that create/enrich/analysis these dataset to generate conclusions. Are the same FAIR condition relevant to research software? What does software that is FAIR mean? % https://www.rd-alliance.org/system/files/FAIR4RS_Principles_v0.3_RDA-RFC.pdf.

% https://www.researchsquare.com/article/rs-1239393/v1

Open source documentation as well as the software itself helps combat unconscious knowledge (knowledge that the initial developer had but forgets that not everyone knows!) Unconcious knowledge can break early stage tasks especially when the primary developer leaves and there is too much of a steep learning curve for anyone to take over.  

% https://www.software.ac.uk/resources/publications/better-software-better-research

\subsection{Why is research software documentation poor?}

The existence of multiple categories of documentation leads to a diverse set of reasons why users may find a software’s documentation insufficient.
To quantitatively detect patterns in poor software documentation \cite{Aghajani2019} scowered mailing lists, github issues and stack overflow for issues posted by users pertaining problems in documentaion. 
Common issues were based on factually incorrect statements in the documentation, sections of code/functions without any documentation at all or documentation becoming out of date with the latest package versions.
Other issues discuss the ease at which API documentation could be found and searched at all, exactly what terms meant in specific contexts and not having quality translations of documentation in other languages.
As expected, a complete lack of documentation is the most common issue but on the other extreme is dense, unintelligible documentation that is difficult to maintain and search.

Every developer knows the importance of documentation, but new software continues to be created with poor documentation.
In a 2017 GitHub survey of OSS contributors, 93\% reported that “incomplete or outdated documentation is a pervasive problem” but “60\% of contributors say they rarely or never contribute to documentation” \cite{Geiger2017}.
Software documentation typically is the least credited part of software development with little time or funds allocated to its development.
Documentation writers are first to go in times of economic difficulty. \cite{Forward2002}
However, paradoxically writing software documentation requires the most diverse set of skills and experiences to enable people from different backgrounds and knowledge to jump right in at an appropriate level \cite{Geiger2018}.
Documentation writers also envelope a larger populous than those with technical programming skills as the software developers themselves can be limited in their ability to write accessible documentation by unconscious knowledge.

The skills and interests of software developers may not naturally align with the demands of writing documentation, but perhaps the issues actually lies in the lack of community between developer, documenters and users.
Rich Bowen, Community Manager for the CentOS project,  says "There’s common wisdom in the open source world: Everybody knows that the documentation is awful, that nobody wants to write it, and that this is just the way things are. But the truth is that there are lots of people who want to write the docs. We just make it too hard for them to participate. So they write articles on Stack Overflow, on their blogs, and third-party forums. Although this can be good, it’s also a great way for worst-practice solutions to bloom and gain momentum. Embracing these people and making them part of the official documentation effort for your project has many advantages." https://labs.quansight.org/blog/2020/03/documentation-as-a-way- to-build-community/

\subsection{What are the benefits of good research software documentation?}
\begin{itemize}
    \item Documentation acts as "a resource for learning and a second role: as an advertisement for the software project" and the current health of a project.\cite{Geiger2018}
    \item Documentation supports the formation of a developer and maintainer community. Shifting knowledge acquisition away from poorly moderated forums such as biostars or stackoverflow. Extends accessibility of code by highlighting unconscious knowledge and even languages.
    \item Maybe a quick look at the top downloaded R packages on bioconductor and if that correlates with quality documentation?
    \item The quality of hardware and the underlying software dependencies of a package will also change and update, requiring regular maintenance of code for it to be continued to use. However, documentation and reporting the motivations/solutions to problems is often a contribution that last far longer.
    \item Archived and outdated software packages are still being used for the alignment of sequencing data because they are well documented. Simply saying a piece of code is old/out of date is not enough. Justification of the severe negatives of using outdated code should be highlighted in the documentation.
    \item As software get more and more specialised and refined we need to come up with a better way of accrediting work and encouraging documentation so we know exactly how the dependencies run and have a chance of fixing any future eventual bugs. (xkcd meme about software dependency and maintenance. https://xkcd.com/2347/)
\end{itemize}

\subsection{How do we improve research software documentation?}

\begin{itemize}
    \item Lessons learnt from R: vignettes, roxygen, pkgdown
    \item Documentation framework to ensure the right documentation is included and maintainable.
    \item Diataxis is a popular framework for creating documentation along its two axis of knowledge: theory vs practice and acquisition vs application. They separate software documentation into four rough types: Tutorials, practical and ap- plication knowledge; How-tos, practical and acquisition knowledge; references, theoretical and acquisition knowledge; and explaintions, theoretical and application knowledge. %https://diataxis.fr/
    \item User interviews to evaluate documentation as well as functionality
    \item Use of GitHub (issue add transparency to decisions, template issues/pull request help contributors)
    \item Including/understanding the pedagogy of software documentation
    \item Resources available on the Turing Way, open life science and Mozilla
\end{itemize}

% https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2015EA000136 https://f1000research.c 295/v1 https://www.exascaleproject.org/event/softwaredocumentation/ https://elib.dlr.de/147388/
% https://headrush.typepad.com/creatingpassionateusers/2006/01/crashcour 
% https://www.proquest.com/docview/304387282
% https://ieeexplore.ieee.org/document/1241364 https://www.jstor.org/stable/44424301
% https://scholar.lib.vt.edu/ejournals/JOTS/Summer- Fall-2000/holmes.html

\bibliographystyle{apalike}
\bibliography{sbc-template}

\end{document}